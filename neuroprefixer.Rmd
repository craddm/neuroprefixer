---
title: "Neuroprefixer"
output: html_notebook
---

```{r loadPackages}
library(rvest)
library(openNLP)
library(NLP)
library(twitteR)
library(readr)
```

I got the idea for this from Molly White's [Cyberprefixer](https://twitter.com/cyberprefixer), which scrapes news headlines and adds the prefix cyber- to random nouns. I stole bits of code from various places: 
http://rpubs.com/Ranthony__/web-scraping-activity-10
https://www.r-bloggers.com/tutorial-how-to-set-up-a-twitter-bot-using-r/

First I define a custom function to tag the parts of speech (i.e. work out which words are nouns). (nicked from http://stackoverflow.com/questions/35772720/tagging-part-of-speech-for-a-particular-word-in-r)

```{r tagPartsOfSpeech}
tagPOS <-  function(x, ...) {
s <- as.String(x)
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
#POStagged <- paste(sprintf("%s/%s", s[a3w], POStags), collapse = " ")
#list(POStagged = POStagged, POStags = POStags)
POStags}
```

Next we do the actual scraping. Here I'm just taking the BBC's most read headlines.

```{r setURLs}
#url to top 10 most read page 
bbc_most_read<- "http://www.bbc.com/news/popular/read"

#base url for bbc website
#we'll need this later to complete our links
bbc_base_url<-"http://www.bbc.com"

#scrape the ranking and article titles as a single list
most_read<-bbc_most_read %>% 
read_html() %>% 
html_nodes(".most-popular-page-list-item span") %>% 
html_text()


#extract the even numbered elements in the list which are all article titles 
title<-most_read[seq(2,20,2)]
title
```

Now the fun part. We select a random headline, tag the parts of speech, pick out nouns that start with lowercase letters, prefix them with neuro-, then combine them back into a sentence.

```{r createNewHeadline }
selectedHeadline <- sample(1:10,1)
oldHeadline <- title[selectedHeadline]
tmpsplit <- unlist(strsplit(oldHeadline," "))
taggedHeadline <- tagPOS(strsplit(gsub("[^[:alnum:] ]","",oldHeadline)," ")[[1]])
nounIndex <- grep("NN",taggedHeadline)
rle(diff(nounIndex))
nouns <- tmpsplit[nounIndex]
lowerIdx <- grep("(^[[:lower:]])",nouns)
nouns <- nouns[lowerIdx]
nounIndex <- nounIndex[lowerIdx]
newNouns <- paste("neuro",nouns,sep = "")

tmp <- tmpsplit
for (i in 1:length(lowerIdx)) {
  if (i == 1) {
    tmp[nounIndex[i]] <- newNouns[i]
  } else if (nounIndex[i] != (nounIndex[i-1]+1)) {
    tmp[nounIndex[i]] <- newNouns[i]
  }
}
#tmp
paste(tmp,collapse = " ")
```

Now to do the twittering...

```{r}
 # set up twitter api
api_keys <- read_csv("~/GitHub/neuroprefixer/twitter_access.csv")
setup_twitter_oauth(consumer_key = api_keys$consumer_key,
                      consumer_secret = api_keys$consumer_secret,
                      access_token = api_keys$access_token,
                      access_secret = api_keys$access_secret)
time <- Sys.time()
  
# create tweet
tweet_text <- paste(tmp,collapse = " ")
  # send tweet
  tweet(tweet_text)
```
