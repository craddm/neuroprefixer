---
title: "Neuroprefixer"
output: html_notebook
---

```{r loadPackages}
library(rvest)
library(openNLP)
library(NLP)
library(twitteR)
library(readr)
source("offensive.R")
```

I got the idea for this from Molly White's [Cyberprefixer](https://twitter.com/cyberprefixer), which scrapes news headlines and adds the prefix cyber- to random nouns. I stole bits of code from various places: 
http://rpubs.com/Ranthony__/web-scraping-activity-10
https://www.r-bloggers.com/tutorial-how-to-set-up-a-twitter-bot-using-r/

First I define a custom function to tag the parts of speech (i.e. work out which words are nouns). (nicked from http://stackoverflow.com/questions/35772720/tagging-part-of-speech-for-a-particular-word-in-r)

```{r tagPartsOfSpeech}
tagPOS <-  function(x, ...) {
  s <- as.String(x)
  word_token_annotator <- Maxent_Word_Token_Annotator()
  a2 <- Annotation(1L, "sentence", 1L, nchar(s))
  a2 <- annotate(s, word_token_annotator, a2)
  a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
  a3w <- a3[a3$type == "word"]
  POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
  POStags}
```

Next we do the actual scraping. Here I get the BBC's Most Read headlines and the latest news from Google UK, then remove headlines which contain one of a list of potentially offensive words. This is to avoid making tasteless headlines.

```{r setURLs}
#URL for the Most Read section of BBC news
bbcMostRead <- "http://www.bbc.com/news/popular/read"

#Scrape the headlines
mostRead <- bbcMostRead %>% 
  read_html() %>% 
  html_nodes(".most-popular-page-list-item span") %>% 
  html_text()

#Extract the article titles (the even elements of the list)
bbcHeadlines <-mostRead[seq(2,20,2)]

#Scrape google's RSS feed for UK/world news
googleRSS <- read_xml("https://news.google.com/news?pz=1&cf=all&ned=uk&hl=en&output=rss") %>%
  html_nodes("title") %>%
  html_text()

#Remove the names of the newspapers/websites the headlines are scraped from, and the two useless titles at the start of the list of headlines
googleHeadlines <-  gsub("( -).*","",googleRSS[3:length(googleRSS)])

#Combine them into a single set of headlines
headlines <- c(googleHeadlines,bbcHeadlines)

#Remove 
headlines <- headlines[grep(paste(bannedWords,collapse="|"),headlines,invert = TRUE,ignore.case = TRUE)]
headlines
```

Now the fun part. We select a random headline, tag the parts of speech, pick out nouns that start with lowercase letters, prefix them with neuro-, then combine them back into a sentence.

```{r createNewHeadline }

#pick out a random headline, split it into single words and then tag those words as nouns etc.
selectedHeadline <- headlines[sample(1:length(headlines),1)]
tmpsplit <- unlist(strsplit(selectedHeadline," "))
taggedHeadline <- tagPOS(strsplit(gsub("[^[:alnum:] ]","",selectedHeadline)," ")[[1]])

#pick out the nouns, select only those that start with a lowercase letter, and add prefix them with neuro
nounIndex <- grep("NN",taggedHeadline)
lowerIdx <- grep("(^[[:lower:]])",tmpsplit[nounIndex])
nounIndex <- nounIndex[lowerIdx]
newNouns <- paste("neuro",tmpsplit[nounIndex],sep = "")

#Replace the old nouns with the new neuronouns and put them back together to make a new headline. This also checks to make sure two consecutive words aren't replaced.
for (i in 1:length(lowerIdx)) {
  if (i == 1) {
    tmpsplit[nounIndex[i]] <- newNouns[i]
  } else if (nounIndex[i] != (nounIndex[i-1]+1)) {
    tmpsplit[nounIndex[i]] <- newNouns[i]
  }
}
paste(tmpsplit,collapse = " ")
nchar(paste(tmpsplit,collapse = " "))
```

Now to do the tweeting...

```{r}
 # set up twitter api
api_keys <- read_csv("~/GitHub/neuroprefixer/twitter_access.csv")
setup_twitter_oauth(consumer_key = api_keys$consumer_key,
                      consumer_secret = api_keys$consumer_secret,
                      access_token = api_keys$access_token,
                      access_secret = api_keys$access_secret)
time <- Sys.time()
  
# create tweet
tweet_text <- paste(tmpsplit,collapse = " ")
# send tweet
tweet(tweet_text)

#will add a log of tweets soon, eventually planning to automate tweeting...
```
